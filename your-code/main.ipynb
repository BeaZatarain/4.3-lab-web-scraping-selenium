{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selenium Doc](https://www.selenium.dev/documentation/)\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `Selenium` and `pandas` are imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install webdriver-manager\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable the options you may need. In the next cell you have an example of them but you can choose to use them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver configuration\n",
    "opciones=Options()\n",
    "\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', False)\n",
    "opciones.headless=False    # si True, no aperece la ventana (headless=no visible)\n",
    "#opciones.add_argument('--start-maximized')         # comienza maximizado\n",
    "#opciones.add_argument('user-data-dir=selenium')    # mantiene las cookies\n",
    "#opciones.add_extension('driver_folder/adblock.crx')       # adblocker\n",
    "#opciones.add_argument('--incognito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ChromeDriverManager().install()     # instala el driver de chrome\n",
    "\n",
    "driver=webdriver.Chrome(PATH, options=opciones)      # abre una venta una de chrome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse, and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://github.com/trending/developers')#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = driver.find_element(By.XPATH, '//*[@id=\"pa-a8m\"]/div[2]/div[1]/div[1]/h1/a').text\n",
    "\n",
    "\n",
    "\n",
    "usuarios = driver.find_element(By.XPATH, '//*[@id=\"pa-a8m\"]/div[2]/div[1]/div[1]/p/a').text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ariel Mashraki',\n",
       " 'Henrik Rydgård',\n",
       " 'jxxghp',\n",
       " 'Shrikant Sharat Kandula',\n",
       " 'Azure SDK Bot',\n",
       " 'Casey Rodarmor',\n",
       " 'Florian Roth',\n",
       " 'Brad Fitzpatrick',\n",
       " 'Emil Ernerfeldt',\n",
       " 'atomiks',\n",
       " 'Jeffrey Su',\n",
       " 'Yair Morgenstern',\n",
       " 'Manuel Kaufmann',\n",
       " 'Paul Razvan Berg',\n",
       " 'Matt Karl',\n",
       " 'nodiscc',\n",
       " 'Matthew Leibowitz',\n",
       " 'Maarten Grootendorst',\n",
       " 'Ben McCann',\n",
       " 'Costa Huang',\n",
       " 'François Chollet',\n",
       " 'Ross Wightman',\n",
       " 'Robert Martin',\n",
       " 'Huon Wilson',\n",
       " 'Zhian N. Kamvar']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = driver.find_elements(By.CLASS_NAME, 'h3.lh-condensed')\n",
    "\n",
    "\n",
    "\n",
    "listanames=[e.text for e in names]\n",
    "\n",
    "listanames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a8m',\n",
       " 'hrydgard',\n",
       " 'jxxghp',\n",
       " 'sharat87',\n",
       " 'azure-sdk',\n",
       " 'casey',\n",
       " 'Neo23x0',\n",
       " 'bradfitz',\n",
       " 'emilk',\n",
       " 'JeffreySu',\n",
       " 'yairm210',\n",
       " 'humitos',\n",
       " 'PaulRBerg',\n",
       " 'bigtimebuddy',\n",
       " 'nodiscc',\n",
       " 'mattleibow',\n",
       " 'MaartenGr',\n",
       " 'benmccann',\n",
       " 'vwxyzjn',\n",
       " 'fchollet',\n",
       " 'rwightman',\n",
       " 'robertmartin8',\n",
       " 'huonw',\n",
       " 'zkamvar']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usuarios = driver.find_elements(By.CLASS_NAME, 'f4.text-normal.mb-1')\n",
    "\n",
    "listausuarios = [e.text for e in usuarios]\n",
    "\n",
    "listausuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ariel Mashraki (a8m)',\n",
       " 'Henrik Rydgård (hrydgard)',\n",
       " 'jxxghp (jxxghp)',\n",
       " 'Shrikant Sharat Kandula (sharat87)',\n",
       " 'Azure SDK Bot (azure-sdk)',\n",
       " 'Casey Rodarmor (casey)',\n",
       " 'Florian Roth (Neo23x0)',\n",
       " 'Brad Fitzpatrick (bradfitz)',\n",
       " 'Emil Ernerfeldt (emilk)',\n",
       " 'atomiks (JeffreySu)',\n",
       " 'Jeffrey Su (yairm210)',\n",
       " 'Yair Morgenstern (humitos)',\n",
       " 'Manuel Kaufmann (PaulRBerg)',\n",
       " 'Paul Razvan Berg (bigtimebuddy)',\n",
       " 'Matt Karl (nodiscc)',\n",
       " 'nodiscc (mattleibow)',\n",
       " 'Matthew Leibowitz (MaartenGr)',\n",
       " 'Maarten Grootendorst (benmccann)',\n",
       " 'Ben McCann (vwxyzjn)',\n",
       " 'Costa Huang (fchollet)',\n",
       " 'François Chollet (rwightman)',\n",
       " 'Ross Wightman (robertmartin8)',\n",
       " 'Robert Martin (huonw)',\n",
       " 'Huon Wilson (zkamvar)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listafinaltrendings = []\n",
    "\n",
    "for e, i in zip(listanames, listausuarios):\n",
    "    listafinaltrendings.append(e + ' (' + i + ')')\n",
    "\n",
    "listafinaltrendings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(PATH, options=opciones)  \n",
    "driver.get('https://github.com/trending/python?since=daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acikkaynak / deprem-yardim-backend',\n",
       " 'TheAlgorithms / Python',\n",
       " 'Zero6992 / chatGPT-discord-bot',\n",
       " 'zhayujie / chatgpt-on-wechat',\n",
       " 'TEXTurePaper / TEXTurePaper',\n",
       " 'acheong08 / ChatGPT',\n",
       " 'LAION-AI / Open-Assistant',\n",
       " 'AIGCT / EASYChatGPT',\n",
       " 'BlinkDL / ChatRWKV',\n",
       " 'ddPn08 / Lsmith',\n",
       " 'd2l-ai / d2l-zh',\n",
       " 'chidiwilliams / buzz',\n",
       " 'arc53 / DocsGPT',\n",
       " 'prowler-cloud / prowler',\n",
       " 'openai / openai-python',\n",
       " 'hwchase17 / langchain',\n",
       " 'karfly / chatgpt_telegram_bot',\n",
       " 'biancangming / wtv',\n",
       " 'LlmKira / Openaibot',\n",
       " 'BlinkDL / RWKV-LM',\n",
       " 'wagtail / wagtail',\n",
       " 'PaddlePaddle / ERNIE',\n",
       " 'MrMimic / data-scientist-roadmap',\n",
       " 'PaddlePaddle / PaddleSpeech',\n",
       " 'bellingcat / octosuite']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repositorios = driver.find_elements(By.CLASS_NAME, 'h3.lh-condensed')\n",
    "\n",
    "listarepositorios = [e.text for e in repositorios]\n",
    "\n",
    "listarepositorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(PATH, options=opciones)  \n",
    "driver.get('https://en.wikipedia.org/wiki/Walt_Disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fotos = driver.find_elements(By.CLASS_NAME, 'image')[3].get_attribute('href')\n",
    "\n",
    "fotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/File:Walt_Disney_1946.JPG',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Steamboat-willie.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_1935.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'https://en.wikipedia.org/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:DisneySchiphol1951.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Disney1968.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Disney_Oscar_1953_(cropped).jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Disneyland_Resort_logo.svg',\n",
       " 'https://en.wikipedia.org/wiki/File:Animation_disc.svg',\n",
       " 'https://en.wikipedia.org/wiki/File:Magic_Kingdom_castle.jpg',\n",
       " 'https://en.wikipedia.org/wiki/File:Blank_television_set.svg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listafotos = [e.get_attribute('href') for e in driver.find_elements(By.CLASS_NAME, 'image')]\n",
    "\n",
    "listafotos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Python'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = driver.find_elements(By.TAG_NAME, 'a')[3].get_attribute('href')\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Wikipedia:Contents'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listalinks = [e.get_attribute('href') for e in driver.find_elements(By.TAG_NAME, 'a')]\n",
    "\n",
    "listalinks[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 2 - The Congress',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 18 - Crimes and Criminal Procedure ٭',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 23 - Highways ٭',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 29 - Labor',\n",
       " 'Title 31 - Money and Finance ٭',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " \"Title 38 - Veterans' Benefits ٭\",\n",
       " 'Title 41 - Public Contracts ٭',\n",
       " 'Title 42 - The Public Health and Welfare']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cambios = driver.find_elements(By.CLASS_NAME, 'usctitlechanged')\n",
    "\n",
    "listacambios = [e.text for e in cambios]\n",
    "\n",
    "listacambios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'RUJA IGNATOVA',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'ALEXIS FLORES',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buscados = driver.find_elements(By.CLASS_NAME, 'title')\n",
    "\n",
    "listabuscados = [e.text for e in buscados]\n",
    "\n",
    "listabuscados[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Citizen\\nResponse',\n",
       " 'Date & Time\\nUTC',\n",
       " 'Latitude\\ndegrees',\n",
       " 'Longitude\\ndegrees',\n",
       " 'Depth\\nkm',\n",
       " 'Mag\\n[+]',\n",
       " 'Region name\\n[+]',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabeceras = driver.find_elements(By.CLASS_NAME, 'th2')\n",
    "\n",
    "litacabeceras = [e.text.strip() for e in cabeceras]\n",
    "\n",
    "litacabeceras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-02-08   18:40:24.2\\n14min ago',\n",
       " '2023-02-08   18:36:45.0\\n17min ago',\n",
       " '2023-02-08   18:32:44.7\\n21min ago',\n",
       " '2023-02-08   18:28:47.1\\n25min ago',\n",
       " '2023-02-08   18:26:12.5\\n28min ago',\n",
       " '2023-02-08   18:22:00.0\\n32min ago',\n",
       " '2023-02-08   18:18:57.9\\n35min ago',\n",
       " '2023-02-08   18:16:30.0\\n38min ago',\n",
       " '2023-02-08   18:12:48.9\\n41min ago',\n",
       " '2023-02-08   18:08:40.0\\n45min ago',\n",
       " '2023-02-08   18:07:54.3\\n46min ago',\n",
       " '2023-02-08   18:06:51.3\\n47min ago',\n",
       " '2023-02-08   18:03:42.8\\n50min ago',\n",
       " '2023-02-08   18:01:07.0\\n53min ago',\n",
       " '2023-02-08   18:00:23.0\\n54min ago',\n",
       " '2023-02-08   17:57:23.9\\n57min ago',\n",
       " '2023-02-08   17:53:28.6\\n1hr 01min ago',\n",
       " '2023-02-08   17:52:07.0\\n1hr 02min ago',\n",
       " '2023-02-08   17:50:15.3\\n1hr 04min ago',\n",
       " '2023-02-08   17:48:09.8\\n1hr 06min ago',\n",
       " '2023-02-08   17:44:47.3\\n1hr 09min ago',\n",
       " '2023-02-08   17:43:00.9\\n1hr 11min ago',\n",
       " '2023-02-08   17:40:20.8\\n1hr 14min ago',\n",
       " '2023-02-08   17:39:05.0\\n1hr 15min ago',\n",
       " '2023-02-08   17:37:47.4\\n1hr 16min ago',\n",
       " '2023-02-08   17:35:34.2\\n1hr 19min ago',\n",
       " '2023-02-08   17:31:51.7\\n1hr 22min ago',\n",
       " '2023-02-08   17:23:46.0\\n1hr 30min ago',\n",
       " '2023-02-08   17:20:57.5\\n1hr 33min ago',\n",
       " '2023-02-08   17:19:43.4\\n1hr 34min ago',\n",
       " '2023-02-08   17:15:20.0\\n1hr 39min ago',\n",
       " '2023-02-08   17:12:52.5\\n1hr 41min ago',\n",
       " '2023-02-08   17:07:43.6\\n1hr 46min ago',\n",
       " '2023-02-08   17:06:39.4\\n1hr 47min ago',\n",
       " '2023-02-08   17:03:58.4\\n1hr 50min ago',\n",
       " '2023-02-08   17:02:32.3\\n1hr 52min ago',\n",
       " '2023-02-08   17:01:12.6\\n1hr 53min ago',\n",
       " '2023-02-08   16:59:20.0\\n1hr 55min ago',\n",
       " '2023-02-08   16:55:39.5\\n1hr 58min ago',\n",
       " '2023-02-08   16:44:28.2\\n2hr 10min ago',\n",
       " '2023-02-08   16:40:33.2\\n2hr 14min ago',\n",
       " '2023-02-08   16:29:07.4\\n2hr 25min ago',\n",
       " '2023-02-08   16:21:19.8\\n2hr 33min ago',\n",
       " '2023-02-08   16:18:17.6\\n2hr 36min ago',\n",
       " '2023-02-08   16:15:05.3\\n2hr 39min ago',\n",
       " '2023-02-08   16:12:04.2\\n2hr 42min ago',\n",
       " '2023-02-08   16:01:57.0\\n2hr 52min ago',\n",
       " '2023-02-08   16:01:04.0\\n2hr 53min ago',\n",
       " '2023-02-08   15:57:02.0\\n2hr 57min ago',\n",
       " '2023-02-08   15:56:50.9\\n2hr 57min ago']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = driver.find_elements(By.CLASS_NAME, 'tabev6')\n",
    "\n",
    "litsadate = [e.text.strip() for e in date]\n",
    "\n",
    "\n",
    "\n",
    "litsadate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['38.07',\n",
       " '24.03',\n",
       " '39.58',\n",
       " '38.09',\n",
       " '37.32',\n",
       " '33.08',\n",
       " '37.87',\n",
       " '42.10',\n",
       " '38.15',\n",
       " '27.80',\n",
       " '44.01',\n",
       " '38.11',\n",
       " '19.16',\n",
       " '28.14',\n",
       " '38.03',\n",
       " '39.81',\n",
       " '37.93',\n",
       " '8.37',\n",
       " '38.10',\n",
       " '2.70',\n",
       " '37.80',\n",
       " '38.46',\n",
       " '38.08',\n",
       " '3.16',\n",
       " '39.22',\n",
       " '38.12',\n",
       " '37.98',\n",
       " '36.05',\n",
       " '38.03',\n",
       " '38.08',\n",
       " '37.86',\n",
       " '38.09',\n",
       " '22.01',\n",
       " '38.87',\n",
       " '37.87',\n",
       " '35.47',\n",
       " '38.09',\n",
       " '37.10',\n",
       " '38.10',\n",
       " '37.97',\n",
       " '37.84',\n",
       " '37.74',\n",
       " '19.15',\n",
       " '38.00',\n",
       " '37.55',\n",
       " '38.10',\n",
       " '4.92',\n",
       " '38.15',\n",
       " '5.69',\n",
       " '37.32']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude = driver.find_elements(By.CLASS_NAME, 'tabev1')\n",
    "\n",
    "latlist= [e.text.strip() for e in latitude]\n",
    "\n",
    "latlist = latlist[0::2]\n",
    "\n",
    "\n",
    "latlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['37.29',\n",
       " '68.88',\n",
       " '118.47',\n",
       " '37.68',\n",
       " '37.07',\n",
       " '72.03',\n",
       " '36.63',\n",
       " '142.80',\n",
       " '36.94',\n",
       " '70.20',\n",
       " '13.32',\n",
       " '37.82',\n",
       " '155.48',\n",
       " '69.06',\n",
       " '36.55',\n",
       " '40.88',\n",
       " '38.10',\n",
       " '77.02',\n",
       " '36.67',\n",
       " '140.60',\n",
       " '36.70',\n",
       " '39.53',\n",
       " '36.56',\n",
       " '126.10',\n",
       " '38.79',\n",
       " '37.10',\n",
       " '38.12',\n",
       " '36.08',\n",
       " '37.54',\n",
       " '37.72',\n",
       " '37.65',\n",
       " '37.40',\n",
       " '179.35',\n",
       " '37.38',\n",
       " '36.72',\n",
       " '3.66',\n",
       " '37.17',\n",
       " '36.74',\n",
       " '37.20',\n",
       " '38.25',\n",
       " '36.60',\n",
       " '37.40',\n",
       " '155.48',\n",
       " '37.51',\n",
       " '37.34',\n",
       " '36.69',\n",
       " '122.66',\n",
       " '38.61',\n",
       " '154.36',\n",
       " '37.07']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitude = driver.find_elements(By.CLASS_NAME, 'tabev1')\n",
    "\n",
    "lonlist= [e.text.strip() for e in longitude]\n",
    "\n",
    "lonlist = lonlist[1::2]\n",
    "\n",
    "lonlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CENTRAL TURKEY',\n",
       " 'ANTOFAGASTA, CHILE',\n",
       " 'NEVADA',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'OFFSHORE VALPARAISO, CHILE',\n",
       " 'CENTRAL TURKEY',\n",
       " 'HOKKAIDO, JAPAN REGION',\n",
       " 'CENTRAL TURKEY',\n",
       " 'ATACAMA, CHILE',\n",
       " 'ADRIATIC SEA',\n",
       " 'CENTRAL TURKEY',\n",
       " 'ISLAND OF HAWAII, HAWAII',\n",
       " 'LA RIOJA, ARGENTINA',\n",
       " 'CENTRAL TURKEY',\n",
       " 'EASTERN TURKEY',\n",
       " 'EASTERN TURKEY',\n",
       " 'PANAMA-COLOMBIA BORDER REGION',\n",
       " 'CENTRAL TURKEY',\n",
       " 'NEAR N COAST OF PAPUA, INDONESIA',\n",
       " 'CENTRAL TURKEY',\n",
       " 'EASTERN TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'KEPULAUAN TALAUD, INDONESIA',\n",
       " 'EASTERN TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'EASTERN TURKEY',\n",
       " 'TURKEY-SYRIA BORDER REGION',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'SOUTH OF FIJI ISLANDS',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'STRAIT OF GIBRALTAR',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'EASTERN TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'ISLAND OF HAWAII, HAWAII',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'CENTRAL TURKEY',\n",
       " 'SULAWESI, INDONESIA',\n",
       " 'EASTERN TURKEY',\n",
       " 'BOUGAINVILLE REGION, P.N.G.',\n",
       " 'CENTRAL TURKEY']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = driver.find_elements(By.CLASS_NAME, 'tb_region')\n",
    "\n",
    "listaregion= [e.text.strip() for e in region]\n",
    "\n",
    "\n",
    "listaregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citizen\\nResponse</th>\n",
       "      <th>Date &amp; Time\\nUTC</th>\n",
       "      <th>Latitude\\ndegrees</th>\n",
       "      <th>Longitude\\ndegrees</th>\n",
       "      <th>Depth\\nkm</th>\n",
       "      <th>Mag\\n[+]</th>\n",
       "      <th>Region name\\n[+]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Citizen\n",
       "Response, Date & Time\n",
       "UTC, Latitude\n",
       "degrees, Longitude\n",
       "degrees, Depth\n",
       "km, Mag\n",
       "[+], Region name\n",
       "[+], ]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=litacabeceras)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citizen\\nResponse</th>\n",
       "      <th>Date &amp; Time\\nUTC</th>\n",
       "      <th>Latitude\\ndegrees</th>\n",
       "      <th>Longitude\\ndegrees</th>\n",
       "      <th>Depth\\nkm</th>\n",
       "      <th>Mag\\n[+]</th>\n",
       "      <th>Region name\\n[+]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-08   18:40:24.2\\n14min ago</td>\n",
       "      <td>38.07</td>\n",
       "      <td>37.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-08   18:36:45.0\\n17min ago</td>\n",
       "      <td>24.03</td>\n",
       "      <td>68.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-08   18:32:44.7\\n21min ago</td>\n",
       "      <td>39.58</td>\n",
       "      <td>118.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEVADA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-08   18:28:47.1\\n25min ago</td>\n",
       "      <td>38.09</td>\n",
       "      <td>37.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-08   18:26:12.5\\n28min ago</td>\n",
       "      <td>37.32</td>\n",
       "      <td>37.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Citizen\\nResponse                    Date & Time\\nUTC Latitude\\ndegrees  \\\n",
       "0               NaN  2023-02-08   18:40:24.2\\n14min ago             38.07   \n",
       "1               NaN  2023-02-08   18:36:45.0\\n17min ago             24.03   \n",
       "2               NaN  2023-02-08   18:32:44.7\\n21min ago             39.58   \n",
       "3               NaN  2023-02-08   18:28:47.1\\n25min ago             38.09   \n",
       "4               NaN  2023-02-08   18:26:12.5\\n28min ago             37.32   \n",
       "\n",
       "  Longitude\\ndegrees Depth\\nkm Mag\\n[+]    Region name\\n[+]       \n",
       "0              37.29       NaN      NaN      CENTRAL TURKEY  NaN  \n",
       "1              68.88       NaN      NaN  ANTOFAGASTA, CHILE  NaN  \n",
       "2             118.47       NaN      NaN              NEVADA  NaN  \n",
       "3              37.68       NaN      NaN      CENTRAL TURKEY  NaN  \n",
       "4              37.07       NaN      NaN      CENTRAL TURKEY  NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[litacabeceras[1]] = litsadate\n",
    "\n",
    "df[litacabeceras[2]] = latlist\n",
    "\n",
    "df[litacabeceras[3]] = lonlist \n",
    "\n",
    "df[litacabeceras[6]] = listaregion\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time\\nUTC</th>\n",
       "      <th>Latitude\\ndegrees</th>\n",
       "      <th>Longitude\\ndegrees</th>\n",
       "      <th>Region name\\n[+]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-08   18:40:24.2\\n14min ago</td>\n",
       "      <td>38.07</td>\n",
       "      <td>37.29</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-08   18:36:45.0\\n17min ago</td>\n",
       "      <td>24.03</td>\n",
       "      <td>68.88</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-08   18:32:44.7\\n21min ago</td>\n",
       "      <td>39.58</td>\n",
       "      <td>118.47</td>\n",
       "      <td>NEVADA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-08   18:28:47.1\\n25min ago</td>\n",
       "      <td>38.09</td>\n",
       "      <td>37.68</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-08   18:26:12.5\\n28min ago</td>\n",
       "      <td>37.32</td>\n",
       "      <td>37.07</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-08   18:22:00.0\\n32min ago</td>\n",
       "      <td>33.08</td>\n",
       "      <td>72.03</td>\n",
       "      <td>OFFSHORE VALPARAISO, CHILE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-02-08   18:18:57.9\\n35min ago</td>\n",
       "      <td>37.87</td>\n",
       "      <td>36.63</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-02-08   18:16:30.0\\n38min ago</td>\n",
       "      <td>42.10</td>\n",
       "      <td>142.80</td>\n",
       "      <td>HOKKAIDO, JAPAN REGION</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-02-08   18:12:48.9\\n41min ago</td>\n",
       "      <td>38.15</td>\n",
       "      <td>36.94</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-02-08   18:08:40.0\\n45min ago</td>\n",
       "      <td>27.80</td>\n",
       "      <td>70.20</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-02-08   18:07:54.3\\n46min ago</td>\n",
       "      <td>44.01</td>\n",
       "      <td>13.32</td>\n",
       "      <td>ADRIATIC SEA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-02-08   18:06:51.3\\n47min ago</td>\n",
       "      <td>38.11</td>\n",
       "      <td>37.82</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-02-08   18:03:42.8\\n50min ago</td>\n",
       "      <td>19.16</td>\n",
       "      <td>155.48</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-02-08   18:01:07.0\\n53min ago</td>\n",
       "      <td>28.14</td>\n",
       "      <td>69.06</td>\n",
       "      <td>LA RIOJA, ARGENTINA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-02-08   18:00:23.0\\n54min ago</td>\n",
       "      <td>38.03</td>\n",
       "      <td>36.55</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-02-08   17:57:23.9\\n57min ago</td>\n",
       "      <td>39.81</td>\n",
       "      <td>40.88</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-02-08   17:53:28.6\\n1hr 01min ago</td>\n",
       "      <td>37.93</td>\n",
       "      <td>38.10</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-02-08   17:52:07.0\\n1hr 02min ago</td>\n",
       "      <td>8.37</td>\n",
       "      <td>77.02</td>\n",
       "      <td>PANAMA-COLOMBIA BORDER REGION</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-02-08   17:50:15.3\\n1hr 04min ago</td>\n",
       "      <td>38.10</td>\n",
       "      <td>36.67</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-02-08   17:48:09.8\\n1hr 06min ago</td>\n",
       "      <td>2.70</td>\n",
       "      <td>140.60</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date & Time\\nUTC Latitude\\ndegrees  \\\n",
       "0       2023-02-08   18:40:24.2\\n14min ago             38.07   \n",
       "1       2023-02-08   18:36:45.0\\n17min ago             24.03   \n",
       "2       2023-02-08   18:32:44.7\\n21min ago             39.58   \n",
       "3       2023-02-08   18:28:47.1\\n25min ago             38.09   \n",
       "4       2023-02-08   18:26:12.5\\n28min ago             37.32   \n",
       "5       2023-02-08   18:22:00.0\\n32min ago             33.08   \n",
       "6       2023-02-08   18:18:57.9\\n35min ago             37.87   \n",
       "7       2023-02-08   18:16:30.0\\n38min ago             42.10   \n",
       "8       2023-02-08   18:12:48.9\\n41min ago             38.15   \n",
       "9       2023-02-08   18:08:40.0\\n45min ago             27.80   \n",
       "10      2023-02-08   18:07:54.3\\n46min ago             44.01   \n",
       "11      2023-02-08   18:06:51.3\\n47min ago             38.11   \n",
       "12      2023-02-08   18:03:42.8\\n50min ago             19.16   \n",
       "13      2023-02-08   18:01:07.0\\n53min ago             28.14   \n",
       "14      2023-02-08   18:00:23.0\\n54min ago             38.03   \n",
       "15      2023-02-08   17:57:23.9\\n57min ago             39.81   \n",
       "16  2023-02-08   17:53:28.6\\n1hr 01min ago             37.93   \n",
       "17  2023-02-08   17:52:07.0\\n1hr 02min ago              8.37   \n",
       "18  2023-02-08   17:50:15.3\\n1hr 04min ago             38.10   \n",
       "19  2023-02-08   17:48:09.8\\n1hr 06min ago              2.70   \n",
       "\n",
       "   Longitude\\ndegrees                  Region name\\n[+]       \n",
       "0               37.29                    CENTRAL TURKEY  NaN  \n",
       "1               68.88                ANTOFAGASTA, CHILE  NaN  \n",
       "2              118.47                            NEVADA  NaN  \n",
       "3               37.68                    CENTRAL TURKEY  NaN  \n",
       "4               37.07                    CENTRAL TURKEY  NaN  \n",
       "5               72.03        OFFSHORE VALPARAISO, CHILE  NaN  \n",
       "6               36.63                    CENTRAL TURKEY  NaN  \n",
       "7              142.80            HOKKAIDO, JAPAN REGION  NaN  \n",
       "8               36.94                    CENTRAL TURKEY  NaN  \n",
       "9               70.20                    ATACAMA, CHILE  NaN  \n",
       "10              13.32                      ADRIATIC SEA  NaN  \n",
       "11              37.82                    CENTRAL TURKEY  NaN  \n",
       "12             155.48          ISLAND OF HAWAII, HAWAII  NaN  \n",
       "13              69.06               LA RIOJA, ARGENTINA  NaN  \n",
       "14              36.55                    CENTRAL TURKEY  NaN  \n",
       "15              40.88                    EASTERN TURKEY  NaN  \n",
       "16              38.10                    EASTERN TURKEY  NaN  \n",
       "17              77.02     PANAMA-COLOMBIA BORDER REGION  NaN  \n",
       "18              36.67                    CENTRAL TURKEY  NaN  \n",
       "19             140.60  NEAR N COAST OF PAPUA, INDONESIA  NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Citizen\\nResponse', 'Depth\\nkm', 'Mag\\n[+]'], axis=1)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/PaquitaSalas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.168 Tweets']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_paquitasalas = driver.find_elements(By.CLASS_NAME, 'css-901oao.css-1hf3ou5.r-14j79pv.r-37j5jr.r-n6v787.r-16dba41.r-1cwl3u0.r-bcqeeo.r-qvutc0')\n",
    "\n",
    "tweets= [e.text for e in tweets_paquitasalas]\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def followers_count(a):\n",
    "\n",
    "    url = f'https://twitter.com/{a}'\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        followers_element = driver.find_element(By.XPATH,\"//a[contains(@data-nav,'followers')]\")\n",
    "\n",
    "        followers_text = followers_element.get_attribute(\"title\")\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        return followers_text\n",
    "    except:\n",
    "        \n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_count('U2') #no consigo que me funcione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(PATH, options=opciones)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Español1 833 000 artículos',\n",
       " 'English6 606 000 articles',\n",
       " 'Русский1 887 000 статей',\n",
       " '日本語1 359 000 記事',\n",
       " 'Deutsch2 764 000 Artikel',\n",
       " 'Français2 488 000 articles',\n",
       " 'Italiano1 792 000 voci',\n",
       " '中文1 331 000 条目 / 條目',\n",
       " 'فارسی947 000 مقاله',\n",
       " 'Polski1 552 000 haseł']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = driver.find_elements(By.CLASS_NAME, 'central-featured-lang')\n",
    "\n",
    "listanum= [e.text.replace('\\n','').replace('+','') for e in num]\n",
    "\n",
    "listanum=listanum[0:11]\n",
    "\n",
    "listanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = driver.find_elements(By.CLASS_NAME, 'govuk-link')\n",
    "\n",
    "listdatasets= [e.text.strip() for e in datasets]\n",
    "\n",
    "listdatasets = listdatasets[4::]\n",
    "\n",
    "listdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language', 'Native speakers\\n(millions)', 'Language family', 'Branch']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas=driver.find_elements(By.CLASS_NAME, 'headerSort') \n",
    "\n",
    "listcols= [e.text.strip() for e in columnas]\n",
    "\n",
    "listcols=listcols[:4] \n",
    "\n",
    "listcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mandarin Chinese\\n(incl. Standard Chinese, but excl. other varieties)',\n",
       "  '920',\n",
       "  'Sino-Tibetan',\n",
       "  'Sinitic'],\n",
       " ['Spanish', '475', 'Indo-European', 'Romance'],\n",
       " ['English', '373', 'Indo-European', 'Germanic'],\n",
       " ['Hindi\\n(excl. Urdu)', '344', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Bengali', '234', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Portuguese', '232', 'Indo-European', 'Romance'],\n",
       " ['Russian', '154', 'Indo-European', 'Balto-Slavic'],\n",
       " ['Japanese', '125', 'Japonic', 'Japanese'],\n",
       " ['Yue Chinese\\n(incl. Cantonese)', '85.2', 'Sino-Tibetan', 'Sinitic'],\n",
       " ['Vietnamese', '84.6', 'Austroasiatic', 'Vietic'],\n",
       " ['Marathi', '83.1', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Telugu', '82.7', 'Dravidian', 'South-Central'],\n",
       " ['Turkish', '82.2', 'Turkic', 'Oghuz'],\n",
       " ['Wu Chinese\\n(incl. Shanghainese)', '81.8', 'Sino-Tibetan', 'Sinitic'],\n",
       " ['Korean', '81.7', 'Koreanic', 'language isolate'],\n",
       " ['French', '79.9', 'Indo-European', 'Romance'],\n",
       " ['Tamil', '78.4', 'Dravidian', 'South'],\n",
       " ['Standard German', '75.6', 'Indo-European', 'Germanic'],\n",
       " ['Egyptian Spoken Arabic\\n(excl. Saʽidi Arabic)',\n",
       "  '74.8',\n",
       "  'Afroasiatic',\n",
       "  'Semitic'],\n",
       " ['Urdu\\n(excl. Hindi)', '70.2', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Javanese', '68.3', 'Austronesian', 'Malayo-Polynesian'],\n",
       " ['Western Punjabi\\n(excl. Eastern Punjabi)',\n",
       "  '66.4',\n",
       "  'Indo-European',\n",
       "  'Indo-Aryan'],\n",
       " ['Italian', '64.8', 'Indo-European', 'Romance'],\n",
       " ['Gujarati', '57.0', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Iranian Persian\\n(excl. Dari and Tajik)',\n",
       "  '56.4',\n",
       "  'Indo-European',\n",
       "  'Iranian'],\n",
       " ['Bhojpuri', '52.3', 'Indo-European', 'Indo-Aryan'],\n",
       " ['Hausa', '50.8', 'Afroasiatic', 'Chadic']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[e.text for e in f.find_elements(By.TAG_NAME, 'td')]  \n",
    "        for f in driver.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers\\n(millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese\\n(incl. Standard Chinese, but...</td>\n",
       "      <td>920</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>475</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>373</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi\\n(excl. Urdu)</td>\n",
       "      <td>344</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese\\n(incl. Cantonese)</td>\n",
       "      <td>85.2</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>84.6</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.7</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South-Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>82.2</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>Oghuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wu Chinese\\n(incl. Shanghainese)</td>\n",
       "      <td>81.8</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Korean</td>\n",
       "      <td>81.7</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>language isolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>French</td>\n",
       "      <td>79.9</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tamil</td>\n",
       "      <td>78.4</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Standard German</td>\n",
       "      <td>75.6</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Egyptian Spoken Arabic\\n(excl. Saʽidi Arabic)</td>\n",
       "      <td>74.8</td>\n",
       "      <td>Afroasiatic</td>\n",
       "      <td>Semitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Urdu\\n(excl. Hindi)</td>\n",
       "      <td>70.2</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Javanese</td>\n",
       "      <td>68.3</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>Malayo-Polynesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Western Punjabi\\n(excl. Eastern Punjabi)</td>\n",
       "      <td>66.4</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Italian</td>\n",
       "      <td>64.8</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Iranian Persian\\n(excl. Dari and Tajik)</td>\n",
       "      <td>56.4</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Iranian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bhojpuri</td>\n",
       "      <td>52.3</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>50.8</td>\n",
       "      <td>Afroasiatic</td>\n",
       "      <td>Chadic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Language  \\\n",
       "0   Mandarin Chinese\\n(incl. Standard Chinese, but...   \n",
       "1                                             Spanish   \n",
       "2                                             English   \n",
       "3                                 Hindi\\n(excl. Urdu)   \n",
       "4                                             Bengali   \n",
       "5                                          Portuguese   \n",
       "6                                             Russian   \n",
       "7                                            Japanese   \n",
       "8                      Yue Chinese\\n(incl. Cantonese)   \n",
       "9                                          Vietnamese   \n",
       "10                                            Marathi   \n",
       "11                                             Telugu   \n",
       "12                                            Turkish   \n",
       "13                   Wu Chinese\\n(incl. Shanghainese)   \n",
       "14                                             Korean   \n",
       "15                                             French   \n",
       "16                                              Tamil   \n",
       "17                                    Standard German   \n",
       "18      Egyptian Spoken Arabic\\n(excl. Saʽidi Arabic)   \n",
       "19                                Urdu\\n(excl. Hindi)   \n",
       "20                                           Javanese   \n",
       "21           Western Punjabi\\n(excl. Eastern Punjabi)   \n",
       "22                                            Italian   \n",
       "23                                           Gujarati   \n",
       "24            Iranian Persian\\n(excl. Dari and Tajik)   \n",
       "25                                           Bhojpuri   \n",
       "26                                              Hausa   \n",
       "\n",
       "   Native speakers\\n(millions) Language family             Branch  \n",
       "0                          920    Sino-Tibetan            Sinitic  \n",
       "1                          475   Indo-European            Romance  \n",
       "2                          373   Indo-European           Germanic  \n",
       "3                          344   Indo-European         Indo-Aryan  \n",
       "4                          234   Indo-European         Indo-Aryan  \n",
       "5                          232   Indo-European            Romance  \n",
       "6                          154   Indo-European       Balto-Slavic  \n",
       "7                          125         Japonic           Japanese  \n",
       "8                         85.2    Sino-Tibetan            Sinitic  \n",
       "9                         84.6   Austroasiatic             Vietic  \n",
       "10                        83.1   Indo-European         Indo-Aryan  \n",
       "11                        82.7       Dravidian      South-Central  \n",
       "12                        82.2          Turkic              Oghuz  \n",
       "13                        81.8    Sino-Tibetan            Sinitic  \n",
       "14                        81.7        Koreanic   language isolate  \n",
       "15                        79.9   Indo-European            Romance  \n",
       "16                        78.4       Dravidian              South  \n",
       "17                        75.6   Indo-European           Germanic  \n",
       "18                        74.8     Afroasiatic            Semitic  \n",
       "19                        70.2   Indo-European         Indo-Aryan  \n",
       "20                        68.3    Austronesian  Malayo-Polynesian  \n",
       "21                        66.4   Indo-European         Indo-Aryan  \n",
       "22                        64.8   Indo-European            Romance  \n",
       "23                        57.0   Indo-European         Indo-Aryan  \n",
       "24                        56.4   Indo-European            Iranian  \n",
       "25                        52.3   Indo-European         Indo-Aryan  \n",
       "26                        50.8     Afroasiatic             Chadic  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = listcols)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Language family', 'Branch'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
